{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import urllib.request\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# set numpy seed for reproducibility\n",
    "seed(1)\n",
    "# set tf seed for reproducibility\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covtype_dataset():\n",
    "    '''Downloads the Cover Type dataset from UCI repository, returning a file handle'''\n",
    "    CURRENT_DIR = os.getcwd()\n",
    "    COVTYPE_FILENAME = 'covtype.data'\n",
    "    COVTYPE_DATA_PATH = os.path.join(CURRENT_DIR, COVTYPE_FILENAME)\n",
    "    COVTYPE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "    if os.path.isfile(COVTYPE_DATA_PATH):\n",
    "        print('Using local cached copy in', COVTYPE_DATA_PATH)\n",
    "    else:\n",
    "        print('Dataset not found locally. Downloading in', COVTYPE_DATA_PATH)\n",
    "        with urllib.request.urlopen(COVTYPE_URL) as response:\n",
    "            with gzip.GzipFile(fileobj=response) as uncompressed, open(COVTYPE_DATA_PATH, 'wb') as out_file:\n",
    "                file_header = uncompressed.read()\n",
    "                out_file.write(file_header)\n",
    "    return COVTYPE_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_file = load_covtype_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covtype = pd.read_csv(covtype_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covtype.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_renaming(df_covtype):\n",
    "    '''Rename each column to meaningful labels'''\n",
    "    # First step: rename the first 14 columns\n",
    "    first_fourteen_old_feature_names = df_covtype.columns[np.arange(0,14)]\n",
    "    first_fourteen_new_feature_names = ['Elevation', 'Aspect', 'Slope',\n",
    "                                        'Horizontal_Distance_To_Hydrology',\n",
    "                                        'Vertical_Distance_To_Hydrology',\n",
    "                                        'Horizontal_Distance_To_Roadways',\n",
    "                                        'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                                        'Horizontal_Distance_To_Fire_Points',\n",
    "                                        'Wilderness_Area_1', 'Wilderness_Area_2',\n",
    "                                        'Wilderness_Area_3', 'Wilderness_Area_4']\n",
    "    old_to_new_name_mapping = dict(zip(first_fourteen_old_feature_names, first_fourteen_new_feature_names))\n",
    "    df_covtype.rename(columns=old_to_new_name_mapping, inplace=True)\n",
    "    # Second step: rename the 40 soil type columns\n",
    "    soil_type_old_feature_names = df_covtype.columns[np.arange(14,54)]\n",
    "    soil_type_new_feature_names = ['Soil_Type_' + str(i) for i in np.arange(1,41)]\n",
    "    old_to_new_name_mapping = dict(zip(soil_type_old_feature_names, soil_type_new_feature_names))\n",
    "    df_covtype.rename(columns=old_to_new_name_mapping, inplace=True)\n",
    "    # Last step: rename the last feature (cover type)\n",
    "    df_covtype.rename(columns={54: 'Cover_Type'}, inplace=True)\n",
    "    return df_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features names are numeric, let's rename each one of them\n",
    "df_covtype = features_renaming(df_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast all features to reduce the overall dataframe dimension\n",
    "# This step is not needed but it keeps the RAM usage low\n",
    "\n",
    "# Get the columns header onto a list\n",
    "df_covtype_headers = df_covtype.columns.values.tolist()\n",
    "# A list containing the new dtype for each column of interest\n",
    "features_new_dtype_list = list()\n",
    "# Iterate through the headers list to assign the new dtype\n",
    "# for each column\n",
    "for i in range(len(df_covtype_headers)):\n",
    "    if i == 3 or i == 4:\n",
    "        # Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology\n",
    "        # could fit in float16\n",
    "        # but float32 works better with mean and std calculations\n",
    "        features_new_dtype_list.append('float32')\n",
    "    elif i < 10:\n",
    "        # First ten features can have int16\n",
    "        features_new_dtype_list.append('int16')\n",
    "    else:\n",
    "        # the remaining ones are binary integers, so uint8\n",
    "        # is enough\n",
    "        features_new_dtype_list.append('uint8')\n",
    "# A dictionary whose keys are the dataframe columns and values\n",
    "# are the new dtype ( namely, {'Elevation' : 'int16', ...} )\n",
    "features_new_dtype_mapping = dict(zip(df_covtype_headers,\n",
    "                                      features_new_dtype_list))\n",
    "# Perform the downcasting using the dictionary just created\n",
    "df_covtype = df_covtype.astype(dtype=features_new_dtype_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of samples for each class value\n",
    "df_covtype.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's copy the first 15120 rows onto a new df and\n",
    "# perform some Exploratory Data Analysis (EDA)\n",
    "train = df_covtype[:15120].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if stratified sampling has already been done for the training set (it has)\n",
    "train.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elevation_multiple_covtype_distplot(elevations_df, covtype_mapping):\n",
    "    '''Plot an Elevation's histogram for each Cover Type on the same figure'''\n",
    "    # Color palette for distplot\n",
    "    custom_palette = ['#F97EDB', '#49C0EA', '#B49EFC', '#CDAB40',\n",
    "                      '#F69089', '#76BF3F', '#4CC9A6']\n",
    "    # Set distplot background\n",
    "    sns.set_style('darkgrid')\n",
    "    # Custom bins range for evenly spaced hists\n",
    "    bins = range(1800,4000,60)\n",
    "    # Iterate through the dictionary to plot a histogram for each cover type\n",
    "    for covtype_id, covtype_name in covtype_mapping.items():\n",
    "        # covtype_id goes from 1 to 7\n",
    "        # thus subtract 1 for indexing custom_palette\n",
    "        palette_idx = covtype_id - 1        \n",
    "        # Create a group for each Cover_Type and return a df satisfying the condition\n",
    "        # on Cover_Type column\n",
    "        by_one_covtype = elevations_df.groupby('Cover_Type') \\\n",
    "                                      .apply(lambda x: x[ x['Cover_Type'] == covtype_id ])\n",
    "        # Plot one Elevation histogram for one group\n",
    "        ax = sns.distplot(by_one_covtype.Elevation,\n",
    "                          bins=bins,\n",
    "                          color=custom_palette[palette_idx], label=covtype_name,\n",
    "                          hist_kws=dict(alpha=0.8, edgecolor=\"none\"),\n",
    "                          kde=False)\n",
    "\n",
    "    # Legend position to upper right\n",
    "    plt.legend(loc='right', bbox_to_anchor=(1.2, 0.8), ncol=1)\n",
    "    # Apply proper labeling to the axes\n",
    "    ax.set(xlabel='Elevation (meters)', ylabel='Count')\n",
    "    # Avoid cutting off the legend from the figure\n",
    "    plt.tight_layout()\n",
    "    # Show the figure (can be omitted in Jupyter Notebooks)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an Elevation's histogram for each Cover Type\n",
    "# to check for possible class separation\n",
    "\n",
    "# Slice by two columns: Elevation and Cover_Type\n",
    "elevations = train.loc[:, ['Elevation', 'Cover_Type']]\n",
    "# Dictionary for mapping each integer target label to its string value\n",
    "covtype_label_name_dict = {1: 'Spruce/Fir',\n",
    "                           2: 'Lodgepole Pine',\n",
    "                           3: 'Ponderosa Pine',\n",
    "                           4: 'Cottonwood/Willow',\n",
    "                           5: 'Aspen',\n",
    "                           6: 'Douglas-fir',\n",
    "                           7: 'Krummholz'}\n",
    "# It is clear that classes\n",
    "# 4 (Willow), 5 (Aspen) and 7 (Krummholz) are easily separable\n",
    "elevation_multiple_covtype_distplot(elevations, covtype_label_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need elevations anymore\n",
    "print('Dereferencing elevations')\n",
    "del elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Elevation looks like a promising feature for Cover Type prediction\n",
    "# let's check the correlation matrix to see which continuos feature\n",
    "# depends on the Elevation\n",
    "# Note that we could have used Chi-Squared Test to check for features\n",
    "# dependency on Cover Type but it is out of scope for this project\n",
    "\n",
    "# The features we are interested in for the correlation matrix\n",
    "features_to_check = ['Elevation', 'Aspect',\n",
    "                     'Slope',\n",
    "                     'Horizontal_Distance_To_Hydrology',\n",
    "                     'Vertical_Distance_To_Hydrology',\n",
    "                     'Horizontal_Distance_To_Roadways',\n",
    "                     'Hillshade_9am', 'Hillshade_Noon',\n",
    "                     'Hillshade_3pm',\n",
    "                     'Horizontal_Distance_To_Fire_Points']\n",
    "# Same features as above, just with some shorter names\n",
    "# for visualization purposes\n",
    "labels_to_plot = ['Elevation', 'Aspect',\n",
    "                  'Slope', 'HD_Hydro',\n",
    "                  'VD_Hydro', 'HD_Road',\n",
    "                  'HS_9am', 'HS_Noon',\n",
    "                  'HS_3pm', 'HD_Fire']\n",
    "# Create a dictionary for mapping the labels like this\n",
    "# {long_label: short_label}\n",
    "shorter_labels = dict(zip(features_to_check, labels_to_plot))\n",
    "# Make a copy of the training set because we want to rename\n",
    "# some columns to fit them on the graph\n",
    "corr_train = train[features_to_check].copy()\n",
    "# Do the renaming feeding the mapping dictionary we created\n",
    "corr_train.rename(columns=shorter_labels, inplace=True)\n",
    "# Grab the AxesSubplots handle to modify labels padding and rotation\n",
    "axes = scatter_matrix(corr_train, figsize=(12, 8))\n",
    "n = len(corr_train.columns)\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        # for all the axes on the graph..\n",
    "        ax = axes[x,y]\n",
    "        # rotate the y-axis labels by 0 (horizontally)..\n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # add some padding between the labels and their subgraph..\n",
    "        ax.xaxis.labelpad = 0\n",
    "        ax.yaxis.labelpad = 20\n",
    "        # and hide axes value ranges\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write down the most correlated features:\n",
    "# - 'HD_Hydro' and 'VD_Hydro'\n",
    "# - 'HS_Noon' and 'HS_3pm'\n",
    "# Plot the same scatter graph zooming on these\n",
    "corr_train.plot(kind='scatter', x='HD_Hydro', y='VD_Hydro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are a lot of HS_3pm equal to zero.\n",
    "# It could help to impute those with the median\n",
    "corr_train.plot(kind='scatter', x='HS_Noon', y='HS_3pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need corr_train df anymore\n",
    "# Hopefully the garbage collector will clean it up\n",
    "print('Dereferencing corr_train')\n",
    "del corr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Hillshade_3pm zeros on the training set\n",
    "(train.Hillshade_3pm == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing Hillshade_3pm zeros with the median\n",
    "simp = SimpleImputer(missing_values=0, strategy='median')\n",
    "# fit_transform requires X as a numpy array of shape [n_samples, n_features]\n",
    "# thus the dataframe column is casted to a numpy array and reshaped\n",
    "train.Hillshade_3pm = simp.fit_transform(train.Hillshade_3pm.values.reshape(-1,1))\n",
    "# The imputer upcast the df column to float64, we don't need that\n",
    "train.Hillshade_3pm = train.Hillshade_3pm.astype(np.float32)\n",
    "# Count the zeros again to check the result\n",
    "(train.Hillshade_3pm == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the training set from df_covtype\n",
    "df_covtype = df_covtype.iloc[15120:]\n",
    "# Append the imputed training set to df_covtype\n",
    "df_covtype = pd.concat([train, df_covtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need train df anymore\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the Euclidean distance to Hydrology\n",
    "hv_distances_labels = ['Horizontal_Distance_To_Hydrology',\n",
    "                       'Vertical_Distance_To_Hydrology']\n",
    "hv_dist_hydro_arr = df_covtype[hv_distances_labels].values\n",
    "# Perform the Euclidean distance with respect to the origin\n",
    "# (i.e (0;0) where the water is located)\n",
    "euc_distance_to_hydro = np.linalg.norm(hv_dist_hydro_arr, axis=1)\n",
    "# Add the new feature 'Distance_To_Hydrology' to the training set\n",
    "# just after the 'Slope' feature at index 2\n",
    "# rounding each distance to four decimal places\n",
    "df_covtype.insert(3,\n",
    "                  'Distance_To_Hydrology',\n",
    "                  np.around(euc_distance_to_hydro, decimals=4))\n",
    "# Drop the horizontal and vertical distance\n",
    "df_covtype.drop(columns=hv_distances_labels, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one hot encoding via get_dummies,\n",
    "# then drop the integer target label, leaving the one hot encoded labels only\n",
    "one_hot_covtype = pd.get_dummies(df_covtype.Cover_Type, prefix='CovT')\n",
    "df_covtype.drop(columns='Cover_Type', inplace=True)\n",
    "df_covtype_ohe = df_covtype.join(one_hot_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting shape\n",
    "df_covtype_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need df_covtype anymore\n",
    "print('Dereferencing df_covtype')\n",
    "del df_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Aspect and Slope since working with angles is tricky\n",
    "# when computing the mean\n",
    "df_covtype_ohe.drop(labels=['Aspect', 'Slope'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df_covtype_ohe):\n",
    "    '''\n",
    "    Split the one hot encoded dataset onto training set and test set\n",
    "    according to UCI's repository guidelines\n",
    "    '''\n",
    "    # First 15120 rows for the training set\n",
    "    X_train = df_covtype_ohe[:15120].copy()\n",
    "    # The last seven colums are the targets\n",
    "    X_train, y_train = X_train.iloc[:, :51], X_train.iloc[:, 51:]\n",
    "    # The remaining rows are for the test set\n",
    "    X_test = df_covtype_ohe[15121:].copy()\n",
    "    X_test, y_test = X_test.iloc[:, :51], X_test.iloc[:, 51:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_covtype_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes consistency\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}, ' \\\n",
    "      f'y_train: {y_train.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's standardize the training set and test set.\n",
    "# Note that we use the training set ONLY to calculate the mean and standard deviation\n",
    "# then normalize the training set \n",
    "# and finally use the (training) mean and standard deviation to normalize the test set.\n",
    "# This ensures no data leakage.\n",
    "\n",
    "def train_test_normalize(X_train, X_test):\n",
    "    '''\n",
    "    Perform standardization on the training set and transforms the\n",
    "    test set accordingly\n",
    "    '''\n",
    "    # The numerical columns we want to normalize\n",
    "    numerical_columns = ['Elevation',\n",
    "                         'Distance_To_Hydrology',\n",
    "                         'Horizontal_Distance_To_Roadways',\n",
    "                         'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                         'Horizontal_Distance_To_Fire_Points']\n",
    "    # Calculate the mean and standard deviation of the training set\n",
    "    X_train_num_cols_mean = X_train[numerical_columns].mean()\n",
    "    X_train_num_cols_std = X_train[numerical_columns].std()\n",
    "    # Perform standardization over the numerical columns of the training set\n",
    "    X_train_std = (X_train[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized training set and the one-hot encoded features\n",
    "    # Note that we index X_train dataframe by the (set) difference of the overall features\n",
    "    # minus the numerical ones\n",
    "    ohe_features = X_train.columns.difference(other=numerical_columns, sort=False)\n",
    "    X_train_std = pd.concat([X_train_std, X_train[ohe_features]], axis=1)\n",
    "    # Perform standardization over the numerical columns of the test set, using the mean and std\n",
    "    # of the training set as discussed earlier\n",
    "    X_test_std = (X_test[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized test set and the one-hot encoded features\n",
    "    X_test_std = pd.concat([X_test_std, X_test[ohe_features]], axis=1)\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "X_train_std, X_test_std = train_test_normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one validation set from the normalized training set.\n",
    "# Since the training set contains 2160 samples for each class,\n",
    "# let's split according to 75% training set / 25% validation set.\n",
    "# This yields 1620 samples for the new training set and 540 for the validation set.\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=0)\n",
    "for train_index, valid_index in sss.split(X_train_std, y_train):\n",
    "    X_train_std_minus_validation = X_train_std.iloc[train_index]\n",
    "    X_validation = X_train_std.iloc[valid_index]\n",
    "    y_train_minus_validation = y_train.iloc[train_index]\n",
    "    y_validation = y_train.iloc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert these df to numpy arrays for later use\n",
    "X_train_std_minus_validation = X_train_std_minus_validation.to_numpy()\n",
    "X_validation = X_validation.to_numpy()\n",
    "y_train_minus_validation = y_train_minus_validation.to_numpy()\n",
    "y_validation = y_validation.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes consistency\n",
    "print(f'X_train_std w/o validation set: {X_train_std_minus_validation.shape} \\n' \\\n",
    "      f'X_validation: {X_validation.shape} \\n' \\\n",
    "      f'y_train w/o validation set: {y_train_minus_validation.shape} \\n' \\\n",
    "      f'y_validation: {y_validation.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RUN THIS CELL ONLY WHEN NEEDED\n",
    "\n",
    "def test_train_validation_stratified(X, y):\n",
    "    '''\n",
    "    Check if stratified training/validation set via StratifiedShuffleSplit works\n",
    "    '''\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    print('Considering target label', y.name)\n",
    "    print('Class Ratio 1 / Total = {0} / {1}'.format(len(y[y == 1]), len(y)))\n",
    "    print('Class Ratio 0 / Total = {0} / {1}'.format(len(y[y == 0]), len(y)))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=0)\n",
    "    for train_index, valid_index in sss.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        #print('X_TRAIN: \\t{0} X_VALID:{1}'.format(X_train, X_valid))\n",
    "        #print('y_TRAIN: \\t{0} y_VALID:{1}\\n'.format(y_train, y_valid))\n",
    "        print('Train = {0} / {1}'.format(len(y_train[y_train==1]), len(y_train)))\n",
    "        print('Validation = {0} / {1}'.format(len(y_valid[y_valid == 1]), len(y_valid)))\n",
    "        \n",
    "test_train_validation_stratified(X_train_std, y_train.CovT_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features for the network input layer\n",
    "n_features = X_train_std_minus_validation.shape[1]\n",
    "# Build the network using Keras API\n",
    "classifier_net = keras.Sequential()\n",
    "# Input layer has n_features size while the hidden layer has 120 fully connected units\n",
    "# using ReLu activation function\n",
    "classifier_net.add(keras.layers.Dense(120, activation='relu', input_dim=n_features))\n",
    "# Output layer has 7 fully connected units using softmax activation function\n",
    "classifier_net.add(keras.layers.Dense(7, activation='softmax'))\n",
    "# Applies Stochastic Gradient Descent, Categorical Crossentropy loss func\n",
    "classifier_net.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "# Check the model architecture\n",
    "classifier_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each bach is 10% (e.g. 1134) of the training set size (e.g. 11340)\n",
    "# Train the network and test it against the validation set\n",
    "clf_output = classifier_net.fit(X_train_std_minus_validation,\n",
    "                                y_train_minus_validation,\n",
    "                                epochs=100,\n",
    "                                batch_size=1134,\n",
    "                                shuffle=True,\n",
    "                                validation_data=(X_validation, y_validation),\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify classifier on the test set\n",
    "# I'm using the validation here just for code testing purposes\n",
    "# TODO: Remember to SUBSTITUTE X_validation with X_test\n",
    "y_pred_validation = classifier_net.predict(X_validation)\n",
    "# Reverse one-hot encoding (i.e going back to categorical variables)\n",
    "# for the predicted targets\n",
    "y_pred_validation_cat = np.argmax(y_pred_validation, axis=1)\n",
    "# Do the same for the true targets\n",
    "y_validation_cat = np.argmax(y_validation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the confusion matrix\n",
    "confusion_matrix(y_validation_cat, y_pred_validation_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class CovtypeDataset(TensorDataset):\n",
    "\n",
    "    def __init__(self, *dataframes):\n",
    "        tensors = (self._df_to_tensor(df) for df in dataframes)\n",
    "        super(CovtypeDataset, self).__init__(*tensors)\n",
    "\n",
    "    def _df_to_tensor(self, df):\n",
    "        if isinstance(df, pd.Series):\n",
    "            df = df.to_frame()\n",
    "        return torch.from_numpy(df.values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data = CovtypeDataset(X_train_std, y_train)\n",
    "test_data = CovtypeDataset(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 15120\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "print('# training samples:', len(train_data))\n",
    "print('# batches:', len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_hidden=120, n_output=7, p_dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.hidden = nn.Linear(n_features, n_hidden)\n",
    "        self.output = nn.Linear(n_hidden, n_output)\n",
    "#        self.network = nn.Sequential(\n",
    "#            # Input layer\n",
    "#            nn.Linear(n_features, n_hidden),\n",
    "#            # First hidden layer\n",
    "#            nn.Sigmoid,\n",
    "#            nn.Dropout(p_dropout),\n",
    "#            # Output layer\n",
    "#            nn.Sigmoid,\n",
    "#            nn.Dropout(p_dropout),\n",
    "#        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = Classifier(n_features=X_train_std.shape[1])\n",
    "clf_criterion = nn.MSELoss()\n",
    "clf_optimizer = optim.SGD(clf.parameters(), lr=0.05, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def pretrain_classifier(clf, data_loader, optimizer, criterion):\n",
    "#     for x, y in data_loader:\n",
    "#         clf.zero_grad()\n",
    "#         p_y = clf(x)\n",
    "#         loss = criterion(p_y, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     return clf\n",
    "\n",
    "\n",
    "N_CLF_EPOCHS = 2\n",
    "\n",
    "for epoch in range(N_CLF_EPOCHS):\n",
    "    # clf = pretrain_classifier(clf, train_loader, clf_optimizer, clf_criterion)\n",
    "    for x, y in train_loader:\n",
    "        predicted_y = clf(x)\n",
    "        loss = clf_criterion(predicted_y, y)\n",
    "        clf_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clf_optimizer.step()\n",
    "        #Accuracy\n",
    "        predicted_y = (predicted_y>0.5).float()\n",
    "        correct = (predicted_y == y).float().sum()\n",
    "        print(\"Epoch {}/{}, Accuracy: {:.3f}\".format(epoch+1,\n",
    "                                                                   N_CLF_EPOCHS,\n",
    "                                                                   #loss.data[0],\n",
    "                                                                   100. * correct/predicted_y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#with torch.no_grad():\n",
    "#    pre_clf_test = clf(test_data.tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sono le probabilità per il primo sample del test set?\n",
    "#pre_clf_test.data.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=0.1, random_state=1, solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Logistic Regression, training accuracy: %.2f%%' % (100 * lr.score(X_train_std, y_train)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
