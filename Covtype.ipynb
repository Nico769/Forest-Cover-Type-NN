{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import urllib.request\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# set numpy seed for reproducibility\n",
    "seed(1)\n",
    "# set tf seed for reproducibility\n",
    "set_random_seed(2)\n",
    "#\n",
    "# Turn off GPU usage for tf\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_covtype_dataset():\n",
    "    '''Downloads the Cover Type dataset from UCI repository, returning a file handle'''\n",
    "    CURRENT_DIR = os.getcwd()\n",
    "    COVTYPE_FILENAME = 'covtype.data'\n",
    "    COVTYPE_DATA_PATH = os.path.join(CURRENT_DIR, COVTYPE_FILENAME)\n",
    "    COVTYPE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "    if os.path.isfile(COVTYPE_DATA_PATH):\n",
    "        print('Using local cached copy in', COVTYPE_DATA_PATH)\n",
    "    else:\n",
    "        print('Dataset not found locally. Downloading in', COVTYPE_DATA_PATH)\n",
    "        with urllib.request.urlopen(COVTYPE_URL) as response:\n",
    "            with gzip.GzipFile(fileobj=response) as uncompressed, open(COVTYPE_DATA_PATH, 'wb') as out_file:\n",
    "                file_header = uncompressed.read()\n",
    "                out_file.write(file_header)\n",
    "    return COVTYPE_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covtype_file = load_covtype_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_covtype = pd.read_csv(covtype_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_covtype.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def features_renaming(df_covtype):\n",
    "    '''Rename each column to meaningful labels'''\n",
    "    # First step: rename the first 14 columns\n",
    "    first_fourteen_old_feature_names = df_covtype.columns[np.arange(0,14)]\n",
    "    first_fourteen_new_feature_names = ['Elevation', 'Aspect', 'Slope',\n",
    "                                        'Horizontal_Distance_To_Hydrology',\n",
    "                                        'Vertical_Distance_To_Hydrology',\n",
    "                                        'Horizontal_Distance_To_Roadways',\n",
    "                                        'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                                        'Horizontal_Distance_To_Fire_Points',\n",
    "                                        'Wilderness_Area_1', 'Wilderness_Area_2',\n",
    "                                        'Wilderness_Area_3', 'Wilderness_Area_4']\n",
    "    old_to_new_name_mapping = dict(zip(first_fourteen_old_feature_names, first_fourteen_new_feature_names))\n",
    "    df_covtype.rename(columns=old_to_new_name_mapping, inplace=True)\n",
    "    # Second step: rename the 40 soil type columns\n",
    "    soil_type_old_feature_names = df_covtype.columns[np.arange(14,54)]\n",
    "    soil_type_new_feature_names = ['Soil_Type_' + str(i) for i in np.arange(1,41)]\n",
    "    old_to_new_name_mapping = dict(zip(soil_type_old_feature_names, soil_type_new_feature_names))\n",
    "    df_covtype.rename(columns=old_to_new_name_mapping, inplace=True)\n",
    "    # Last step: rename the last feature (cover type)\n",
    "    df_covtype.rename(columns={54: 'Cover_Type'}, inplace=True)\n",
    "    return df_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features names are numeric, let's rename each one of them\n",
    "df_covtype = features_renaming(df_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downcast all features to reduce the overall dataframe dimension\n",
    "# This step is not needed but it keeps the RAM usage low\n",
    "\n",
    "# Get the columns header onto a list\n",
    "df_covtype_headers = df_covtype.columns.values.tolist()\n",
    "# A list containing the new dtype for each column of interest\n",
    "features_new_dtype_list = list()\n",
    "# Iterate through the headers list to assign the new dtype\n",
    "# for each column\n",
    "for i in range(len(df_covtype_headers)):\n",
    "    if i == 3 or i == 4:\n",
    "        # Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology\n",
    "        # could fit in float16\n",
    "        # but float32 works better with mean and std calculations\n",
    "        features_new_dtype_list.append('float32')\n",
    "    elif i < 10:\n",
    "        # First ten features can have int16\n",
    "        features_new_dtype_list.append('int16')\n",
    "    else:\n",
    "        # the remaining ones are binary integers, so uint8\n",
    "        # is enough\n",
    "        features_new_dtype_list.append('uint8')\n",
    "# A dictionary whose keys are the dataframe columns and values\n",
    "# are the new dtype ( namely, {'Elevation' : 'int16', ...} )\n",
    "features_new_dtype_mapping = dict(zip(df_covtype_headers,\n",
    "                                      features_new_dtype_list))\n",
    "# Perform the downcasting using the dictionary just created\n",
    "df_covtype = df_covtype.astype(dtype=features_new_dtype_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of samples for each class value\n",
    "df_covtype.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's copy the first 15120 rows onto a new df and\n",
    "# perform some Exploratory Data Analysis (EDA)\n",
    "train = df_covtype[:15120].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if stratified sampling has already been done for the training set (it has)\n",
    "train.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def elevation_multiple_covtype_distplot(elevations_df, covtype_mapping):\n",
    "    '''Plot an Elevation's histogram for each Cover Type on the same figure'''\n",
    "    # Color palette for distplot\n",
    "    custom_palette = ['#F97EDB', '#49C0EA', '#B49EFC', '#CDAB40',\n",
    "                      '#F69089', '#76BF3F', '#4CC9A6']\n",
    "    # Set distplot background\n",
    "    sns.set_style('darkgrid')\n",
    "    # Custom bins range for evenly spaced hists\n",
    "    bins = range(1800,4000,60)\n",
    "    # Iterate through the dictionary to plot a histogram for each cover type\n",
    "    for covtype_id, covtype_name in covtype_mapping.items():\n",
    "        # covtype_id goes from 1 to 7\n",
    "        # thus subtract 1 for indexing custom_palette\n",
    "        palette_idx = covtype_id - 1        \n",
    "        # Create a group for each Cover_Type and return a df satisfying the condition\n",
    "        # on Cover_Type column\n",
    "        by_one_covtype = elevations_df.groupby('Cover_Type') \\\n",
    "                                      .apply(lambda x: x[ x['Cover_Type'] == covtype_id ])\n",
    "        # Plot one Elevation histogram for one group\n",
    "        ax = sns.distplot(by_one_covtype.Elevation,\n",
    "                          bins=bins,\n",
    "                          color=custom_palette[palette_idx], label=covtype_name,\n",
    "                          hist_kws=dict(alpha=0.8, edgecolor=\"none\"),\n",
    "                          kde=False)\n",
    "\n",
    "    # Legend position to upper right\n",
    "    plt.legend(loc='right', bbox_to_anchor=(1.2, 0.8), ncol=1)\n",
    "    # Apply proper labeling to the axes\n",
    "    ax.set(xlabel='Elevation (meters)', ylabel='Count')\n",
    "    # Avoid cutting off the legend from the figure\n",
    "    plt.tight_layout()\n",
    "    # Show the figure (can be omitted in Jupyter Notebooks)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot an Elevation's histogram for each Cover Type\n",
    "# to check for possible class separation\n",
    "\n",
    "# Slice by two columns: Elevation and Cover_Type\n",
    "elevations = train.loc[:, ['Elevation', 'Cover_Type']]\n",
    "# Dictionary for mapping each integer target label to its string value\n",
    "covtype_label_name_dict = {1: 'Spruce/Fir',\n",
    "                           2: 'Lodgepole Pine',\n",
    "                           3: 'Ponderosa Pine',\n",
    "                           4: 'Cottonwood/Willow',\n",
    "                           5: 'Aspen',\n",
    "                           6: 'Douglas-fir',\n",
    "                           7: 'Krummholz'}\n",
    "# It is clear that classes\n",
    "# 4 (Willow), 5 (Aspen) and 7 (Krummholz) are easily separable\n",
    "elevation_multiple_covtype_distplot(elevations, covtype_label_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't need elevations anymore\n",
    "print('Dereferencing elevations')\n",
    "del elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since the Elevation looks like a promising feature for Cover Type prediction\n",
    "# let's check the correlation matrix to see which continuos feature\n",
    "# depends on the Elevation\n",
    "# Note that we could have used Chi-Squared Test to check for features\n",
    "# dependency on Cover Type but it is out of scope for this project\n",
    "\n",
    "# The features we are interested in for the correlation matrix\n",
    "features_to_check = ['Elevation', 'Aspect',\n",
    "                     'Slope',\n",
    "                     'Horizontal_Distance_To_Hydrology',\n",
    "                     'Vertical_Distance_To_Hydrology',\n",
    "                     'Horizontal_Distance_To_Roadways',\n",
    "                     'Hillshade_9am', 'Hillshade_Noon',\n",
    "                     'Hillshade_3pm',\n",
    "                     'Horizontal_Distance_To_Fire_Points']\n",
    "# Same features as above, just with some shorter names\n",
    "# for visualization purposes\n",
    "labels_to_plot = ['Elevation', 'Aspect',\n",
    "                  'Slope', 'HD_Hydro',\n",
    "                  'VD_Hydro', 'HD_Road',\n",
    "                  'HS_9am', 'HS_Noon',\n",
    "                  'HS_3pm', 'HD_Fire']\n",
    "# Create a dictionary for mapping the labels like this\n",
    "# {long_label: short_label}\n",
    "shorter_labels = dict(zip(features_to_check, labels_to_plot))\n",
    "# Make a copy of the training set because we want to rename\n",
    "# some columns to fit them on the graph\n",
    "corr_train = train[features_to_check].copy()\n",
    "# Do the renaming feeding the mapping dictionary we created\n",
    "corr_train.rename(columns=shorter_labels, inplace=True)\n",
    "# Grab the AxesSubplots handle to modify labels padding and rotation\n",
    "axes = scatter_matrix(corr_train, figsize=(12, 8))\n",
    "n = len(corr_train.columns)\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        # for all the axes on the graph..\n",
    "        ax = axes[x,y]\n",
    "        # rotate the y-axis labels by 0 (horizontally)..\n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # add some padding between the labels and their subgraph..\n",
    "        ax.xaxis.labelpad = 0\n",
    "        ax.yaxis.labelpad = 20\n",
    "        # and hide axes value ranges\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's write down the most correlated features:\n",
    "# - 'HD_Hydro' and 'VD_Hydro'\n",
    "# - 'HS_Noon' and 'HS_3pm'\n",
    "# Plot the same scatter graph zooming on these\n",
    "corr_train.plot(kind='scatter', x='HD_Hydro', y='VD_Hydro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looks like there are a lot of HS_3pm equal to zero.\n",
    "# It could help to impute those with the median\n",
    "corr_train.plot(kind='scatter', x='HS_Noon', y='HS_3pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't need corr_train df anymore\n",
    "# Hopefully the garbage collector will clean it up\n",
    "print('Dereferencing corr_train')\n",
    "del corr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count Hillshade_3pm zeros on the training set\n",
    "(train.Hillshade_3pm == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imputing Hillshade_3pm zeros with the median\n",
    "simp = SimpleImputer(missing_values=0, strategy='median')\n",
    "# fit_transform requires X as a numpy array of shape [n_samples, n_features]\n",
    "# thus the dataframe column is casted to a numpy array and reshaped\n",
    "train.Hillshade_3pm = simp.fit_transform(train.Hillshade_3pm.values.reshape(-1,1))\n",
    "# The imputer upcast the df column to float64, we don't need that\n",
    "train.Hillshade_3pm = train.Hillshade_3pm.astype(np.float32)\n",
    "# Count the zeros again to check the result\n",
    "(train.Hillshade_3pm == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the training set from df_covtype\n",
    "df_covtype = df_covtype.iloc[15120:]\n",
    "# Append the imputed training set to df_covtype\n",
    "df_covtype = pd.concat([train, df_covtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't need train df anymore\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's compute the Euclidean distance to Hydrology\n",
    "hv_distances_labels = ['Horizontal_Distance_To_Hydrology',\n",
    "                       'Vertical_Distance_To_Hydrology']\n",
    "hv_dist_hydro_arr = df_covtype[hv_distances_labels].values\n",
    "# Perform the Euclidean distance with respect to the origin\n",
    "# (i.e (0;0) where the water is located)\n",
    "euc_distance_to_hydro = np.linalg.norm(hv_dist_hydro_arr, axis=1)\n",
    "# Add the new feature 'Distance_To_Hydrology' to the training set\n",
    "# just after the 'Slope' feature at index 2\n",
    "# rounding each distance to four decimal places\n",
    "df_covtype.insert(3,\n",
    "                  'Distance_To_Hydrology',\n",
    "                  np.around(euc_distance_to_hydro, decimals=4))\n",
    "# Drop the horizontal and vertical distance\n",
    "df_covtype.drop(columns=hv_distances_labels, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform one hot encoding via get_dummies,\n",
    "# then drop the integer target label, leaving the one hot encoded labels only\n",
    "one_hot_covtype = pd.get_dummies(df_covtype.Cover_Type, prefix='CovT')\n",
    "df_covtype.drop(columns='Cover_Type', inplace=True)\n",
    "df_covtype_ohe = df_covtype.join(one_hot_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the resulting shape\n",
    "df_covtype_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't need df_covtype anymore\n",
    "print('Dereferencing df_covtype')\n",
    "del df_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop Aspect and Slope since working with angles is tricky\n",
    "# when computing the mean\n",
    "df_covtype_ohe.drop(labels=['Aspect', 'Slope'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(df_covtype_ohe):\n",
    "    '''\n",
    "    Split the one hot encoded dataset onto training set and test set\n",
    "    according to UCI's repository guidelines\n",
    "    '''\n",
    "    # First 15120 rows for the training set\n",
    "    X_train = df_covtype_ohe[:15120].copy()\n",
    "    # The last seven colums are the targets\n",
    "    X_train, y_train = X_train.iloc[:, :51], X_train.iloc[:, 51:]\n",
    "    # The remaining rows are for the test set\n",
    "    X_test = df_covtype_ohe[15121:].copy()\n",
    "    X_test, y_test = X_test.iloc[:, :51], X_test.iloc[:, 51:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_covtype_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check shapes consistency\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}, ' \\\n",
    "      f'y_train: {y_train.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's standardize the training set and test set.\n",
    "# Note that we use the training set ONLY to calculate the mean and standard deviation\n",
    "# then normalize the training set \n",
    "# and finally use the (training) mean and standard deviation to normalize the test set.\n",
    "# This ensures no data leakage.\n",
    "\n",
    "def train_test_normalize(X_train, X_test):\n",
    "    '''\n",
    "    Perform standardization on the training set and transforms the\n",
    "    test set accordingly\n",
    "    '''\n",
    "    # The numerical columns we want to normalize\n",
    "    numerical_columns = ['Elevation',\n",
    "                         'Distance_To_Hydrology',\n",
    "                         'Horizontal_Distance_To_Roadways',\n",
    "                         'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                         'Horizontal_Distance_To_Fire_Points']\n",
    "    # Calculate the mean and standard deviation of the training set\n",
    "    X_train_num_cols_mean = X_train[numerical_columns].mean()\n",
    "    X_train_num_cols_std = X_train[numerical_columns].std()\n",
    "    # Perform standardization over the numerical columns of the training set\n",
    "    X_train_std = (X_train[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized training set and the one-hot encoded features\n",
    "    # Note that we index X_train dataframe by the (set) difference of the overall features\n",
    "    # minus the numerical ones\n",
    "    ohe_features = X_train.columns.difference(other=numerical_columns, sort=False)\n",
    "    X_train_std = pd.concat([X_train_std, X_train[ohe_features]], axis=1)\n",
    "    # Perform standardization over the numerical columns of the test set, using the mean and std\n",
    "    # of the training set as discussed earlier\n",
    "    X_test_std = (X_test[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized test set and the one-hot encoded features\n",
    "    X_test_std = pd.concat([X_test_std, X_test[ohe_features]], axis=1)\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "X_train_std, X_test_std = train_test_normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert train/test sets to numpy ndarrays\n",
    "X_train_std = X_train_std.to_numpy()\n",
    "X_test_std = X_test_std.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the training/validation set splitter.\n",
    "# We'll use it later for the grid-search, performing a 10-fold cross validation\n",
    "# where each validation set is 25% the size of the training set.\n",
    "# This yields 1620 samples per class for the new training set and\n",
    "# 540 samples per class for each validation set.\n",
    "validation_strat = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Subclass KerasClassifier and override fit() method\n",
    "# to fix OOM error when running sklearn GridSearchCV on the GPU\n",
    "class GridKerasClassifier(keras.wrappers.scikit_learn.KerasClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        # Clear tensorflow session each time fit is invoked\n",
    "        keras.backend.clear_session()\n",
    "        # Use custom configuration for the new session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.Session(config=config)\n",
    "        keras.backend.set_session(session)\n",
    "        super().fit(*args, **kwargs)\n",
    "\n",
    "# Wrapper function which builds the classifier architecture\n",
    "def make_model(n_features=51, n_classes=7, dense_layer_size=120, learning_rate=0.5):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(dense_layer_size, activation='relu', input_dim=n_features))\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "    sgd = keras.optimizers.SGD(lr=learning_rate) # per il momentum: momentum=0.9\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras neural network onto a scikit Classifier\n",
    "classifier_net = GridKerasClassifier(make_model)\n",
    "\n",
    "# For params tuning see\n",
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "# Define the grid search params\n",
    "# uncomment for changing the default number of features and/or classes\n",
    "# for the network. DON'T FORGET TO CHANGE THE TRAINING/TEST SET ACCORDINGLY!\n",
    "# n_features = [X_train_std.shape[1]]\n",
    "# n_classes = [y_train.shape[1]]\n",
    "dense_layer_size = [120]\n",
    "# 0.5 is the best so far\n",
    "learning_rate = [0.5]\n",
    "# 128 seems to work well\n",
    "batch_size = [64, 128, 256]\n",
    "# Try 100, 500, 1000\n",
    "epochs = [100, 200]\n",
    "# Define the grid of parameters \n",
    "param_grid = dict(# n_features=n_features, # uncomment if needed\n",
    "                  # n_classes=n_classes,   # uncomment if needed\n",
    "                  dense_layer_size=dense_layer_size,\n",
    "                  learning_rate=learning_rate,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs)\n",
    "# Initialize the grid search using the nn classifier and the cross-validation\n",
    "# strategy defined above. Since clear_session() is invoked after each CV run,\n",
    "# it's fine to set n_jobs=-1 when running on the GPU.\n",
    "grid = GridSearchCV(estimator=classifier_net,\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    cv=validation_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "# THIS CELL IS TIME CONSUMING, BE AWARE OF THAT!\n",
    "grid.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the best scores defined earlier for the grid search and its model params\n",
    "print(f'Best scores: {grid.best_score_:.4f} using {grid.best_params_}', end='\\n\\n')\n",
    "# Get the mean of each score for each cross-validation run\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "# Get the standard dev of each score for each cross-validation run\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "# Get the model params for each cross-validation run\n",
    "params = grid.cv_results_['params']\n",
    "# Loop through means, stds, params and print\n",
    "# one line for each cross-validation run\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Mean: {mean:.4f} +- {stdev:.6f} with: {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that the grid search is done\n",
    "# let's generate ONE validation set using the same 75/25% split as before\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=1)\n",
    "for train_index, valid_index in sss.split(X_train_std, y_train):\n",
    "    X_train_std_minus_validation = X_train_std[train_index]\n",
    "    X_validation = X_train_std[valid_index]\n",
    "    y_train_minus_validation = y_train[train_index]\n",
    "    y_validation = y_train[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check shapes consistency\n",
    "print(f'X_train_std w/o validation set: {X_train_std_minus_validation.shape} \\n' \\\n",
    "      f'X_validation: {X_validation.shape} \\n' \\\n",
    "      f'y_train w/o validation set: {y_train_minus_validation.shape} \\n' \\\n",
    "      f'y_validation: {y_validation.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RUN THIS CELL ONLY WHEN NEEDED\n",
    "\n",
    "def test_train_validation_stratified(X, y):\n",
    "    '''\n",
    "    Check if stratified training/validation set via StratifiedShuffleSplit works.\n",
    "    \n",
    "    Args:\n",
    "    X (Dataframe): the normalized training set\n",
    "    y (Dataframe): the training set class labels\n",
    "    '''\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    print('Considering target label', y.name)\n",
    "    print('Class Ratio 1 / Total = {0} / {1}'.format(len(y[y == 1]), len(y)))\n",
    "    print('Class Ratio 0 / Total = {0} / {1}'.format(len(y[y == 0]), len(y)))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=2)\n",
    "    for train_index, valid_index in sss.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        #print('X_TRAIN: \\t{0} X_VALID:{1}'.format(X_train, X_valid))\n",
    "        #print('y_TRAIN: \\t{0} y_VALID:{1}\\n'.format(y_train, y_valid))\n",
    "        print('Train = {0} / {1}'.format(len(y_train[y_train==1]), len(y_train)))\n",
    "        print('Validation = {0} / {1}'.format(len(y_valid[y_valid == 1]), len(y_valid)))\n",
    "        \n",
    "test_train_validation_stratified(X_train_std, y_train.CovT_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify the classifier on the validation set\n",
    "best_classifier_net = grid.best_estimator_.model\n",
    "y_pred_validation = best_classifier_net.predict(X_validation)\n",
    "# Reverse one-hot encoding (i.e going back to categorical variables)\n",
    "# for the predicted targets\n",
    "# Note that argmax return indexes starting from 0, hence add 1\n",
    "y_pred_validation_cat = np.argmax(y_pred_validation, axis=1) + 1\n",
    "# Do the same for the true targets\n",
    "y_true_validation_cat = np.argmax(y_validation, axis=1) + 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We can use sklearn.metrics for several useful metrics\n",
    "# since we have y_pred and y_true\n",
    "target_names = ['Spruce/Fir', 'Lodgepole Pine',\n",
    "                'Ponderosa Pine', 'Cottonwood/Willow',\n",
    "                'Aspen', 'Douglas-fir', 'Krummholz']\n",
    "\n",
    "print(classification_report(y_true_validation_cat,\n",
    "                            y_pred_validation_cat,\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run this block when model training is done, DON'T CHEAT!\n",
    "# TODO: questa cella è un casino, refactorizzare in base\n",
    "# a ciò fatto più su per il validation set\n",
    "# #####################################################\n",
    "# Get the unwrapper (keras) model\n",
    "#best_model = grid.best_estimator_.model\n",
    "# Call Keras .predict and return the predicted targets\n",
    "#y_pred = best_model.predict(X_test_std)\n",
    "# Get keras metrics passed to make_model\n",
    "#metric_names = best_model.metrics_names\n",
    "# Evaluate the best model on the test set and\n",
    "# then print its metrics and values\n",
    "#metric_values = best_model.evaluate(X_test_std, y_test)\n",
    "#for metric, value in zip(metric_names, metric_values):\n",
    "#    print(f'Metric: {metric}, value: {value}')\n",
    "# Reverse one-hot encoding (i.e going back to categorical variables)\n",
    "# for the predicted targets\n",
    "# Note that argmax return indexes starting from 0, hence add 1\n",
    "#y_pred_cat = np.argmax(y_pred, axis=1) + 1\n",
    "# Do the same for the true targets\n",
    "#y_test_cat = np.argmax(y_test, axis=1) + 1\n",
    "# Non normalized confusion matrix\n",
    "#cm = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "#print(cm)\n",
    "# Normalized confusion matrix see\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# #####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=0.1, random_state=1, solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Logistic Regression, training accuracy: %.2f%%' % (100 * lr.score(X_train_std, y_train)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
