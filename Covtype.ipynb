{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "import urllib.request\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "def load_covtype_dataset():\n",
    "    '''Downloads the Cover Type dataset from UCI repository, returning a file handle'''\n",
    "    CURRENT_DIR = os.getcwd()\n",
    "    COVTYPE_FILENAME = 'covtype.data'\n",
    "    COVTYPE_DATA_PATH = os.path.join(CURRENT_DIR, COVTYPE_FILENAME)\n",
    "    COVTYPE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "    if os.path.isfile(COVTYPE_DATA_PATH):\n",
    "        print('Using local cached copy in', COVTYPE_DATA_PATH)\n",
    "    else:\n",
    "        print('Dataset not found locally. Downloading in', COVTYPE_DATA_PATH)\n",
    "        with urllib.request.urlopen(COVTYPE_URL) as response:\n",
    "            with gzip.GzipFile(fileobj=response) as uncompressed, open(COVTYPE_DATA_PATH, 'wb') as out_file:\n",
    "                file_header = uncompressed.read()\n",
    "                out_file.write(file_header)\n",
    "    return COVTYPE_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_file = load_covtype_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_covtype = pd.read_csv(covtype_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covtype.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_renaming(df_covtype):\n",
    "    '''Rename each column to meaningful labels'''\n",
    "    # First step: rename the first 14 columns\n",
    "    first_fourteen_old_feature_names = df_covtype.columns[np.arange(0,14)]\n",
    "    first_fourteen_new_feature_names = ['Elevation', 'Aspect', 'Slope',\n",
    "                                        'Horizontal_Distance_To_Hydrology',\n",
    "                                        'Vertical_Distance_To_Hydrology',\n",
    "                                        'Horizontal_Distance_To_Roadways',\n",
    "                                        'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                                        'Horizontal_Distance_To_Fire_Points',\n",
    "                                        'Wilderness_Area_1', 'Wilderness_Area_2',\n",
    "                                        'Wilderness_Area_3', 'Wilderness_Area_4']\n",
    "    old_to_new_name_mapping = dict(zip(first_fourteen_old_feature_names, first_fourteen_new_feature_names))\n",
    "    df_covtype.rename(columns=old_to_new_name_mapping, inplace=True)\n",
    "    # Second step: rename the 40 soil type columns\n",
    "    soil_type_old_feature_names = df_covtype.columns[np.arange(14,54)]\n",
    "    soil_type_new_feature_names = ['Soil_Type_' + str(i) for i in np.arange(1,41)]\n",
    "    old_to_new_name_mapping = dict(zip(soil_type_old_feature_names, soil_type_new_feature_names))\n",
    "    df_covtype.rename(columns=old_to_new_name_mapping, inplace=True)\n",
    "    # Last step: rename the last feature (cover type)\n",
    "    df_covtype.rename(columns={54: 'Cover_Type'}, inplace=True)\n",
    "    return df_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features names are numeric, let's rename each one of them\n",
    "df_covtype = features_renaming(df_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of samples for each class value\n",
    "df_covtype.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's copy the first 15120 rows onto a new df and\n",
    "# perform some Exploratory Data Analysis (EDA)\n",
    "train = df_covtype[:15120].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if stratified sampling has already been done for the training set (it has)\n",
    "train.Cover_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def elevation_multiple_covtype_distplot(elevations_df, covtype_mapping):\n",
    "    '''Plot an Elevation's histogram for each Cover Type on the same figure'''\n",
    "    # Color palette for distplot\n",
    "    custom_palette = ['#F97EDB', '#49C0EA', '#B49EFC', '#CDAB40',\n",
    "                      '#F69089', '#76BF3F', '#4CC9A6']\n",
    "    # Set distplot background\n",
    "    sns.set_style('darkgrid')\n",
    "    # Custom bins range for evenly spaced hists\n",
    "    bins = range(1800,4000,60)\n",
    "    # Iterate through the dictionary to plot a histogram for each cover type\n",
    "    for covtype_id, covtype_name in covtype_mapping.items():\n",
    "        # covtype_id goes from 1 to 7\n",
    "        # thus subtract 1 for indexing custom_palette\n",
    "        palette_idx = covtype_id - 1        \n",
    "        # Create a group for each Cover_Type and return a df satisfying the condition\n",
    "        # on Cover_Type column\n",
    "        by_one_covtype = elevations_df.groupby('Cover_Type') \\\n",
    "                                      .apply(lambda x: x[ x['Cover_Type'] == covtype_id ])\n",
    "        # Plot one Elevation histogram for one group\n",
    "        ax = sns.distplot(by_one_covtype.Elevation,\n",
    "                          bins=bins,\n",
    "                          color=custom_palette[palette_idx], label=covtype_name,\n",
    "                          hist_kws=dict(alpha=0.8, edgecolor=\"none\"),\n",
    "                          kde=False)\n",
    "\n",
    "    # Legend position to upper right\n",
    "    plt.legend(loc='right', bbox_to_anchor=(1.2, 0.8), ncol=1)\n",
    "    # Apply proper labeling to the axes\n",
    "    ax.set(xlabel='Elevation (meters)', ylabel='Count')\n",
    "    # Avoid cutting off the legend from the figure\n",
    "    plt.tight_layout()\n",
    "    # Show the figure (can be omitted in Jupyter Notebooks)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an Elevation's histogram for each Cover Type\n",
    "# to check for possible class separation\n",
    "\n",
    "# Slice by two columns: Elevation and Cover_Type\n",
    "elevations = train.loc[:, ['Elevation', 'Cover_Type']]\n",
    "# Dictionary for mapping each integer target label to its string value\n",
    "covtype_label_name_dict = {1: 'Spruce/Fir',\n",
    "                           2: 'Lodgepole Pine',\n",
    "                           3: 'Ponderosa Pine',\n",
    "                           4: 'Cottonwood/Willow',\n",
    "                           5: 'Aspen',\n",
    "                           6: 'Douglas-fir',\n",
    "                           7: 'Krummholz'}\n",
    "# It is clear that classes\n",
    "# 4 (Willow), 5 (Aspen) and 7 (Krummholz) are easily separable\n",
    "elevation_multiple_covtype_distplot(elevations, covtype_label_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need elevations anymore\n",
    "print('Dereferencing elevations')\n",
    "del elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Elevation looks like a promising feature for Cover Type prediction\n",
    "# let's check the correlation matrix to see which continuos feature\n",
    "# depends on the Elevation\n",
    "# Note that we could have used Chi-Squared Test to check for features\n",
    "# dependency on Cover Type but it is out of scope for this project\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# The features we are interested in for the correlation matrix\n",
    "features_to_check = ['Elevation', 'Aspect',\n",
    "                     'Slope',\n",
    "                     'Horizontal_Distance_To_Hydrology',\n",
    "                     'Vertical_Distance_To_Hydrology',\n",
    "                     'Horizontal_Distance_To_Roadways',\n",
    "                     'Hillshade_9am', 'Hillshade_Noon',\n",
    "                     'Hillshade_3pm',\n",
    "                     'Horizontal_Distance_To_Fire_Points']\n",
    "# Same features as above, just with some shorter names\n",
    "# for visualization purposes\n",
    "labels_to_plot = ['Elevation', 'Aspect',\n",
    "                  'Slope', 'HD_Hydro',\n",
    "                  'VD_Hydro', 'HD_Road',\n",
    "                  'HS_9am', 'HS_Noon',\n",
    "                  'HS_3pm', 'HD_Fire']\n",
    "# Create a dictionary for mapping the labels like this\n",
    "# {long_label: short_label}\n",
    "shorter_labels = dict(zip(features_to_check, labels_to_plot))\n",
    "# Make a copy of the training set because we want to rename\n",
    "# some columns to fit them on the graph\n",
    "corr_train = train[features_to_check].copy()\n",
    "# Do the renaming feeding the mapping dictionary we created\n",
    "corr_train.rename(columns=shorter_labels, inplace=True)\n",
    "# Grab the AxesSubplots handle to modify labels padding and rotation\n",
    "axes = scatter_matrix(corr_train, figsize=(12, 8))\n",
    "n = len(corr_train.columns)\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        # for all the axes on the graph..\n",
    "        ax = axes[x,y]\n",
    "        # rotate the y-axis labels by 0 (horizontally)..\n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # add some padding between the labels and their subgraph..\n",
    "        ax.xaxis.labelpad = 0\n",
    "        ax.yaxis.labelpad = 20\n",
    "        # and hide axes value ranges\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write down the most correlated features:\n",
    "# - 'HD_Hydro' and 'VD_Hydro'\n",
    "# - 'HS_Noon' and 'HS_3pm'\n",
    "# Plot the same scatter graph zooming on these\n",
    "corr_train.plot(kind='scatter', x='HD_Hydro', y='VD_Hydro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are a lot of HS_3pm equal to zero.\n",
    "# It could help to impute those with the median\n",
    "corr_train.plot(kind='scatter', x='HS_Noon', y='HS_3pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need corr_train df anymore\n",
    "# Hopefully the garbage collector will clean it up\n",
    "print('Dereferencing corr_train')\n",
    "del corr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Hillshade_3pm zeros on the training set\n",
    "(train.Hillshade_3pm == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing Hillshade_3pm zeros with the median\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simp = SimpleImputer(missing_values=0, strategy='median')\n",
    "# fit_transform requires X as a numpy array of shape [n_samples, n_features]\n",
    "# thus the dataframe column is casted to a numpy array and reshaped\n",
    "train.Hillshade_3pm = simp.fit_transform(train.Hillshade_3pm.values.reshape(-1,1))\n",
    "# Count the zeros again to check the result\n",
    "(train.Hillshade_3pm == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the training set from df_covtype\n",
    "df_covtype = df_covtype.iloc[15120:]\n",
    "# Append the imputed training set to df_covtype\n",
    "df_covtype = pd.concat([train, df_covtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need train df anymore\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the Euclidean distance to Hydrology\n",
    "hv_distances_labels = ['Horizontal_Distance_To_Hydrology',\n",
    "                       'Vertical_Distance_To_Hydrology']\n",
    "hv_dist_hydro_arr = df_covtype[hv_distances_labels].values\n",
    "# Perform the Euclidean distance with respect to the origin\n",
    "# (i.e (0;0) where the water is located)\n",
    "euc_distance_to_hydro = np.linalg.norm(hv_dist_hydro_arr, axis=1)\n",
    "# Add the new feature 'Distance_To_Hydrology' to the training set\n",
    "# just after the 'Slope' feature at index 2\n",
    "# rounding each distance to four decimal places\n",
    "df_covtype.insert(3,\n",
    "                  'Distance_To_Hydrology',\n",
    "                  np.around(euc_distance_to_hydro, decimals=4))\n",
    "# Drop the horizontal and vertical distance\n",
    "df_covtype.drop(columns=hv_distances_labels, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one hot encoding via get_dummies,\n",
    "# then drop the integer target label, leaving the one hot encoded labels only\n",
    "one_hot_covtype = pd.get_dummies(df_covtype.Cover_Type, prefix='CovT')\n",
    "df_covtype.drop(columns='Cover_Type', inplace=True)\n",
    "df_covtype_ohe = df_covtype.join(one_hot_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting shape\n",
    "df_covtype_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need df_covtype anymore\n",
    "print('Dereferencing df_covtype')\n",
    "del df_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast all features to reduce the overall dataframe dimension\n",
    "\n",
    "# Get the columns header onto a list\n",
    "df_covtype_ohe_headers = df_covtype_ohe.columns.values.tolist()\n",
    "# There is no need to convert the last 7 elements, they are already uint8\n",
    "# just drop them\n",
    "df_covtype_ohe_headers = df_covtype_ohe_headers[:-7 or None]\n",
    "# A list containing the new dtype for each column of interest\n",
    "features_new_dtype_list = list()\n",
    "# Iterate through the headers list to assign the new dtype\n",
    "# for each column\n",
    "for i in range(len(df_covtype_ohe_headers)):\n",
    "    if i == 3:\n",
    "        # Distance_To_Hydrology could fit in float16\n",
    "        # but float32 works better with mean and std calculations\n",
    "        features_new_dtype_list.append('float32')\n",
    "    elif i < 9:\n",
    "        # First nine features except Distance_To_Hydrology\n",
    "        # can have int16\n",
    "        features_new_dtype_list.append('int16')\n",
    "    else:\n",
    "        # the remaining ones are binary integers, so uint8\n",
    "        # is enough\n",
    "        features_new_dtype_list.append('uint8')\n",
    "# A dictionary whose keys are the dataframe columns and values\n",
    "# are the new dtype ( namely, {'Elevation' : 'int16', ...} )\n",
    "features_new_dtype_mapping = dict(zip(df_covtype_ohe_headers,\n",
    "                                      features_new_dtype_list))\n",
    "# Perform the downcasting using the dictionary just created\n",
    "df_covtype_ohe = df_covtype_ohe.astype(dtype=features_new_dtype_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "df_covtype_ohe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Aspect and Slope since working with angles is tricky\n",
    "# when computing the mean\n",
    "df_covtype_ohe.drop(labels=['Aspect', 'Slope'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df_covtype_ohe):\n",
    "    '''\n",
    "    Split the one hot encoded dataset onto training set and test set\n",
    "    according to UCI's repository guidelines\n",
    "    '''\n",
    "    # First 15120 rows for the training set\n",
    "    X_train = df_covtype_ohe[:15120].copy()\n",
    "    # The last seven colums are the targets\n",
    "    X_train, y_train = X_train.iloc[:, :51], X_train.iloc[:, 51:]\n",
    "    # The remaining rows are for the test set\n",
    "    X_test = df_covtype_ohe[15121:].copy()\n",
    "    X_test, y_test = X_test.iloc[:, :51], X_test.iloc[:, 51:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_covtype_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes consistency\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}, ' \\\n",
    "      f'y_train: {y_train.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's standardize the training set and test set.\n",
    "# Note that we use the training set ONLY to calculate the mean and standard deviation\n",
    "# then normalize the training set \n",
    "# and finally use the (training) mean and standard deviation to normalize the test set.\n",
    "# This ensures no data leakage.\n",
    "\n",
    "def train_test_normalize(X_train, X_test):\n",
    "    '''\n",
    "    Perform standardization on the training set and transforms the\n",
    "    test set accordingly\n",
    "    '''\n",
    "    # The numerical columns we want to normalize\n",
    "    numerical_columns = ['Elevation',\n",
    "                         'Distance_To_Hydrology',\n",
    "                         'Horizontal_Distance_To_Roadways',\n",
    "                         'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                         'Horizontal_Distance_To_Fire_Points']\n",
    "    # Calculate the mean and standard deviation of the training set\n",
    "    X_train_num_cols_mean = X_train[numerical_columns].mean()\n",
    "    X_train_num_cols_std = X_train[numerical_columns].std()\n",
    "    # Perform standardization over the numerical columns of the training set\n",
    "    X_train_std = (X_train[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized training set and the one-hot encoded features\n",
    "    # Note that we index X_train dataframe by the (set) difference of the overall features\n",
    "    # minus the numerical ones\n",
    "    ohe_features = X_train.columns.difference(other=numerical_columns, sort=False)\n",
    "    X_train_std = pd.concat([X_train_std, X_train[ohe_features]], axis=1)\n",
    "    # Perform standardization over the numerical columns of the test set, using the mean and std\n",
    "    # of the training set as discussed earlier\n",
    "    X_test_std = (X_test[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized test set and the one-hot encoded features\n",
    "    X_test_std = pd.concat([X_test_std, X_test[ohe_features]], axis=1)\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "X_train_std, X_test_std = train_test_normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=0.1, random_state=1, solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Logistic Regression, training accuracy: %.2f%%' % (100 * lr.score(X_train_std, y_train)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
