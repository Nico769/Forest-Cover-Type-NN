{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "from datetime import datetime\n",
    "\n",
    "# Disable those annoying warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# Set numpy seed\n",
    "seed(1)\n",
    "# Set tf seed\n",
    "set_random_seed(2)\n",
    "#\n",
    "# Turn off GPU usage for tf\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the previously saved dataframe\n",
    "df_covtype_ohe = pd.read_csv(os.path.join(os.getcwd(), 'covtype_ohe.csv'), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df_covtype_ohe):\n",
    "    '''\n",
    "    Split the one hot encoded dataset onto training set and test set\n",
    "    according to UCI's repository guidelines\n",
    "    '''\n",
    "    # First 15120 rows for the training set\n",
    "    X_train = df_covtype_ohe[:15120].copy()\n",
    "    # The last seven colums are the targets\n",
    "    X_train, y_train = X_train.iloc[:, :51], X_train.iloc[:, 51:]\n",
    "    # The remaining rows are for the test set\n",
    "    X_test = df_covtype_ohe[15120:].copy()\n",
    "    X_test, y_test = X_test.iloc[:, :51], X_test.iloc[:, 51:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_covtype_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes consistency\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}, ' \\\n",
    "      f'y_train: {y_train.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's standardize the training set and test set.\n",
    "# Note that we use the training set ONLY to calculate the mean and standard deviation\n",
    "# then normalize the training set \n",
    "# and finally use the (training) mean and standard deviation to normalize the test set.\n",
    "# This ensures no data leakage.\n",
    "\n",
    "def train_test_normalize(X_train, X_test):\n",
    "    '''\n",
    "    Perform standardization on the training set and transforms the\n",
    "    test set accordingly\n",
    "    '''\n",
    "    # The numerical columns we want to normalize\n",
    "    numerical_columns = ['Elevation',\n",
    "                         'Distance_To_Hydrology',\n",
    "                         'Horizontal_Distance_To_Roadways',\n",
    "                         'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "                         'Horizontal_Distance_To_Fire_Points']\n",
    "    # Calculate the mean and standard deviation of the training set\n",
    "    X_train_num_cols_mean = X_train[numerical_columns].mean()\n",
    "    X_train_num_cols_std = X_train[numerical_columns].std()\n",
    "    # Perform standardization over the numerical columns of the training set\n",
    "    X_train_std = (X_train[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized training set and the one-hot encoded features\n",
    "    # Note that we index X_train dataframe by the (set) difference of the overall features\n",
    "    # minus the numerical ones\n",
    "    ohe_features = X_train.columns.difference(other=numerical_columns, sort=False)\n",
    "    X_train_std = pd.concat([X_train_std, X_train[ohe_features]], axis=1)\n",
    "    # Perform standardization over the numerical columns of the test set, using the mean and std\n",
    "    # of the training set as discussed earlier\n",
    "    X_test_std = (X_test[numerical_columns] - X_train_num_cols_mean) / X_train_num_cols_std\n",
    "    # Concatenate side-by-side the normalized test set and the one-hot encoded features\n",
    "    X_test_std = pd.concat([X_test_std, X_test[ohe_features]], axis=1)\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "X_train_std, X_test_std = train_test_normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train/test sets to numpy float32 ndarrays\n",
    "# (after doing some research it appears that float32 should be used when using a GPU)\n",
    "X_train_std = X_train_std.to_numpy().astype(np.float32)\n",
    "X_test_std = X_test_std.to_numpy().astype(np.float32)\n",
    "y_train = y_train.to_numpy().astype(np.float32)\n",
    "y_test = y_test.to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training/validation set splitter.\n",
    "# We'll use it later for the grid-search, performing a 10-fold cross validation\n",
    "# where each validation set is 25% the size of the training set.\n",
    "# This yields 1620 samples per class for the new training set and\n",
    "# 540 samples per class for each validation set.\n",
    "validation_strat = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass KerasClassifier and override fit() method\n",
    "# to fix OOM error when running sklearn GridSearchCV on the GPU\n",
    "class GridKerasClassifier(keras.wrappers.scikit_learn.KerasClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        # Clear tensorflow session each time fit is invoked\n",
    "        keras.backend.clear_session()\n",
    "        # Use custom configuration for the new session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.Session(config=config)\n",
    "        keras.backend.set_session(session)\n",
    "        super().fit(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function which builds the classifier architecture of the baseline model.\n",
    "def baseline_model(hidden_layers=0, h1_size=120, h2_size=0,\n",
    "                   n_features=51, n_classes=7, learning_rate=0.5):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(h1_size, activation='sigmoid', input_dim=n_features))\n",
    "    model.add(keras.layers.Dense(n_classes, activation='sigmoid'))\n",
    "    sgd = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function which builds the classifier architecture of the model\n",
    "# similar to the baseline one.\n",
    "def crossentropy_model(hidden_layers=0, h1_size=120, h2_size=0,\n",
    "                       n_features=51, n_classes=7, learning_rate=0.5):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(h1_size, activation='relu', input_dim=n_features))\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "    sgd = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function which builds the classifier architecture of a two hidden layer model\n",
    "def two_hidden_model(hidden_layers=2, h1_size=30, h2_size=15,\n",
    "                       n_features=51, n_classes=7, learning_rate=0.3):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(h1_size, activation='relu', input_dim=n_features))\n",
    "    model.add(keras.layers.Dense(h2_size, activation='relu'))\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "    sgd = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottleneck_model(hidden_layers=3, h1_size=46, h2_size=30, h3_size=15,\n",
    "                      n_features=51, n_classes=7, learning_rate=0.3):\n",
    "    model = keras.Sequential()\n",
    "    # autoencoder component begins here\n",
    "    model.add(keras.layers.Dense(h1_size, activation='relu', input_dim=n_features))\n",
    "    model.add(keras.layers.Dense(n_features, activation='relu'))\n",
    "    # autoencoder component ends here\n",
    "    # Adds the two_hidden network right after the autoencoder\n",
    "    model.add(keras.layers.Dense(h2_size, activation='relu'))\n",
    "    model.add(keras.layers.Dense(h3_size, activation='relu'))\n",
    "    model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "    sgd = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the Keras neural network onto a sklearn Classifier\n",
    "net_to_gs = GridKerasClassifier(multi_hidden_model)\n",
    "\n",
    "# For params tuning see\n",
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "\n",
    "# Define the grid search params\n",
    "# uncomment for changing the default number of features and/or classes\n",
    "# for the network. DON'T FORGET TO CHANGE THE TRAINING/TEST SET ACCORDINGLY!\n",
    "# n_features = [X_train_std.shape[1]]\n",
    "# n_classes = [y_train.shape[1]]\n",
    "hidden_layers = [1]\n",
    "h1_size = [30]\n",
    "h2_size = [15]\n",
    "learning_rate = [0.3]\n",
    "batch_size = [128]\n",
    "epochs = [50, 100]\n",
    "# Define the grid of parameters \n",
    "param_grid = dict(# n_features=n_features, # uncomment if needed\n",
    "                  # n_classes=n_classes,   # uncomment if needed\n",
    "                  hidden_layers=hidden_layers,\n",
    "                  h1_size=h1_size,\n",
    "                  h2_size=h2_size,\n",
    "                  learning_rate=learning_rate,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs)\n",
    "# Initialize the grid search using the nn classifier and the cross-validation\n",
    "# strategy defined above. Since clear_session() is invoked after each CV run,\n",
    "# it's fine to set n_jobs=-1 when running on the GPU.\n",
    "grid = GridSearchCV(estimator=net_to_gs,\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    cv=validation_strat,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "# THIS CELL IS TIME CONSUMING, BE AWARE OF THAT!\n",
    "grid.fit(X_train_std, y_train)\n",
    "# Store the best model in the current dir\n",
    "#now = datetime.now()\n",
    "#timestamp = now.strftime('%b-%d-%Y_%H%M')\n",
    "#best_gr_model_params = grid.best_params_\n",
    "#best_gr_model_hlyrs = best_gr_model_params['hidden_layers']\n",
    "#best_gr_model_h1size = best_gr_model_params['h1_size']\n",
    "#best_gr_model_h2size = best_gr_model_params['h2_size']\n",
    "#best_gr_model_epochs = best_gr_model_params['epochs']\n",
    "#best_gr_model_lr = best_gr_model_params['learning_rate']\n",
    "#best_gr_model_bs = best_gr_model_params['batch_size']\n",
    "#best_model_fname = f'hlyrs{best_gr_model_hlyrs}_h1size{best_gr_model_h1size}_h2size{best_gr_model_h2size}' \\\n",
    "#                   f'_epochs{best_gr_model_epochs}_learning_r{best_gr_model_lr}_bsize{best_gr_model_bs}_{timestamp}.hdf5'\n",
    "#best_model_path = os.path.join(os.getcwd(), best_model_fname)\n",
    "#keras.models.save_model(model=grid.best_estimator_.model,\n",
    "#                        filepath=best_model_path,\n",
    "#                        save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the best scores defined earlier for the grid search and its model params\n",
    "print(f'Best scores: {grid.best_score_:.4f} using {grid.best_params_}', end='\\n\\n')\n",
    "# Get the mean of each score for each cross-validation run\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "# Get the standard dev of each score for each cross-validation run\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "# Get the model params for each cross-validation run\n",
    "params = grid.cv_results_['params']\n",
    "# Loop through means, stds, params and print\n",
    "# one line for each cross-validation run\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Mean: {mean:.4f} +- {stdev:.6f} with: {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some cleanup\n",
    "del grid\n",
    "del net_to_gs\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you already have a trained model\n",
    "#model_filename = 'hlyrs2_h1size30_h2size15_epochs200_bsize128_Jul-15-2019_0932.hdf5'\n",
    "#model_to_load_path = os.path.join(os.getcwd(), model_filename)\n",
    "#loaded_model = tf.keras.models.load_model(model_to_load_path)\n",
    "# Uncomment this block to plot a diagram of the model\n",
    "#model_filename_no_ext = os.path.splitext(model_filename)[0]\n",
    "#model_plot_filename = f'{model_filename_no_ext}.png'\n",
    "#keras.utils.plot_model(loaded_model, to_file=model_plot_filename, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the grid search is done\n",
    "# let's retrain the best model via 10-fold cross validation.\n",
    "\n",
    "# Using plain KerasClassifier since our subclass failed returning a History object\n",
    "selected_model = bottleneck_model() # call the desired model func() \n",
    "cv_train_acc = []\n",
    "cv_val_acc = []\n",
    "cv_train_loss = []\n",
    "cv_val_loss = []\n",
    "best_epochs = 100\n",
    "best_batch_size = 128\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=1)\n",
    "cv_fold_counter = 0\n",
    "for train_index, valid_index in sss.split(X_train_std, y_train):\n",
    "    X_train_std_minus_validation = X_train_std[train_index]\n",
    "    X_validation = X_train_std[valid_index]\n",
    "    y_train_minus_validation = y_train[train_index]\n",
    "    y_validation = y_train[valid_index]\n",
    "    history = selected_model.fit(X_train_std_minus_validation,\n",
    "                                 y_train_minus_validation,\n",
    "                                 epochs=best_epochs,\n",
    "                                 batch_size=best_batch_size,\n",
    "                                 validation_data=(X_validation, y_validation),\n",
    "                                 verbose=0)\n",
    "    cv_train_acc.append(history.history['acc'])\n",
    "    cv_val_acc.append(history.history['val_acc'])\n",
    "    cv_train_loss.append(history.history['loss'])\n",
    "    cv_val_loss.append(history.history['val_loss'])\n",
    "    cv_fold_counter += 1\n",
    "    print(f'{cv_fold_counter}-fold completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each epoch, k=10 observations per metric are computed.\n",
    "# For example, cv_train_acc shape is (n_kfolds, n_epochs).\n",
    "# Thus, flatten acc/loss arrays and strip some values for plotting them later\n",
    "# First the acc arrays\n",
    "cv_train_acc_arr = np.array(cv_train_acc).flatten()\n",
    "cv_val_acc_arr = np.array(cv_val_acc).flatten()\n",
    "cv_train_acc_arr = cv_train_acc_arr[0:10*best_epochs:best_epochs]\n",
    "cv_val_acc_arr = cv_val_acc_arr[0:10*best_epochs:best_epochs]\n",
    "# Then the loss arrays\n",
    "cv_train_loss_arr = np.array(cv_train_loss).flatten()\n",
    "cv_val_loss_arr = np.array(cv_val_loss).flatten()\n",
    "cv_train_loss_arr = cv_train_loss_arr[0:10*best_epochs:best_epochs]\n",
    "cv_val_loss_arr = cv_val_loss_arr[0:10*best_epochs:best_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation accuracy vs number of folds for the 10-fold cross validated model\n",
    "plt.figure()\n",
    "plt.plot(range(1, cv_train_acc_arr.shape[0] + 1), cv_train_acc_arr, 'bo', label='Training acc')\n",
    "plt.plot(range(1, cv_val_acc_arr.shape[0] + 1), cv_val_acc_arr, 'b', label='Validation acc')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.title('Two hidden model train/val accuracy - 10-fold cv')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of folds')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation loss vs number of folds for the 10-fold cross validated model\n",
    "plt.figure()\n",
    "plt.plot(range(1, cv_train_loss_arr.shape[0] + 1), cv_train_loss_arr, 'bo', label='Training loss')\n",
    "plt.plot(range(1, cv_val_loss_arr.shape[0] + 1), cv_val_loss_arr, 'b', label='Validation loss')\n",
    "plt.title('Two hidden model train/val loss - 10-fold cv')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of folds')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we are satisfied about the best model run predict on the test set.\n",
    "y_pred = selected_model.predict(X_test_std)\n",
    "# Reverse one-hot encoding (i.e going back to categorical variables) for the predicted targets.\n",
    "y_pred_cat = np.argmax(y_pred, axis=1)\n",
    "# Do the same for the true targets\n",
    "y_true_cat = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have y_pred and y_true, we can use sklearn.metrics for precision, recall and f1-score metrics\n",
    "target_names = ['Spruce/Fir', 'Lodgepole Pine',\n",
    "                'Ponderosa Pine', 'Cottonwood/Willow',\n",
    "                'Aspen', 'Douglas-fir', 'Krummholz']\n",
    "print(classification_report(y_true_cat, y_pred_cat, target_names=target_names))\n",
    "\n",
    "# Uncomment this block to save the classification report to a csv file.\n",
    "#report = classification_report(y_true_cat, y_pred_cat, target_names=target_names, output_dict=True)\n",
    "#df_report = pd.DataFrame(report).transpose()\n",
    "#df_report.to_csv(os.path.join(os.getcwd(), f'clsreptwohidden.csv'), float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptue the non normalized confusion matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_true_cat, y_pred_cat)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "heatmap = sns.heatmap(cm, fmt='g', cmap='Blues', annot=True, ax = ax)\n",
    "ax.set_xlabel('Predicted class')\n",
    "ax.set_ylabel('True class')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "# Save the heatmap to file\n",
    "#heatmapfig = heatmap.get_figure()\n",
    "#heatmapfig.savefig(f'confmat.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
